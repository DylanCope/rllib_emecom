Failure # 1 (occurred at 2023-10-08_16-54-19)
The actor died because of an error raised in its creation task, [36mray::PPO.__init__()[39m (pid=24252, ip=172.28.68.98, actor_id=d7f06a2ed95967f6c8b515ac01000000, repr=PPO)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py", line 227, in _setup
    self.add_workers(
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py", line 593, in add_workers
    raise result.get()
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/actor_manager.py", line 481, in __fetch_result
    result = ray.get(r)
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=24535, ip=172.28.68.98, actor_id=35c4ef7b9a8143cdd14d14f501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc6f7be8a00>)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py", line 525, in __init__
    self._update_policy_map(policy_dict=self.policy_dict)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py", line 1727, in _update_policy_map
    self._build_policy_map(
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py", line 1838, in _build_policy_map
    new_policy = create_policy_for_framework(
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/policy.py", line 142, in create_policy_for_framework
    return policy_class(observation_space, action_space, merged_config)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py", line 64, in __init__
    self._initialize_loss_from_dummy_batch()
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/policy.py", line 1418, in _initialize_loss_from_dummy_batch
    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/torch_policy_v2.py", line 551, in compute_actions_from_input_dict
    return self._compute_action_helper(
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/threading.py", line 24, in wrapper
    return func(self, *a, **k)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/torch_policy_v2.py", line 1217, in _compute_action_helper
    action_dist = action_dist_class.from_logits(dist_inputs)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/models/distributions.py", line 228, in from_logits
    distribution = parent_cls.from_logits(logits, **merged_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/models/torch/torch_distributions.py", line 522, in from_logits
    split_logits = torch.split(logits, logit_lens, dim=1)
  File "/usr/local/lib/python3.10/dist-packages/torch/functional.py", line 189, in split
    return tensor.split(split_size_or_sections, dim)
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 788, in split
    return torch._VF.split_with_sizes(self, split_size, dim)
RuntimeError: split_with_sizes expects split_sizes to sum exactly to 5 (input tensor's size at dimension 1), but got split_sizes=[5, 5, 5]

During handling of the above exception, another exception occurred:

[36mray::PPO.__init__()[39m (pid=24252, ip=172.28.68.98, actor_id=d7f06a2ed95967f6c8b515ac01000000, repr=PPO)
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py", line 517, in __init__
    super().__init__(
  File "/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py", line 185, in __init__
    self.setup(copy.deepcopy(self.config))
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py", line 639, in setup
    self.workers = WorkerSet(
  File "/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py", line 179, in __init__
    raise e.args[0].args[2]
RuntimeError: split_with_sizes expects split_sizes to sum exactly to 5 (input tensor's size at dimension 1), but got split_sizes=[5, 5, 5]
