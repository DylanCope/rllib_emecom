{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1.0\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def one_hot(x, n):\n",
    "    z = np.zeros((x.shape[0], n))\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i, x[i]] = 1\n",
    "    return z\n",
    "\n",
    "data = np.random.random((4, 8))\n",
    "n_classes = 10\n",
    "y = one_hot(np.random.randint(0, n_classes, (data.shape[0],)), n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.11141843, 0.55237528, 0.85151549, 0.71874714, 0.66772003,\n",
       "         0.52591008, 0.88956325, 0.82083697],\n",
       "        [0.89945012, 0.59236676, 0.23607209, 0.60670265, 0.1502762 ,\n",
       "         0.45755856, 0.43141032, 0.50547597],\n",
       "        [0.01076259, 0.86810899, 0.67119655, 0.46519228, 0.50549821,\n",
       "         0.11878284, 0.60170483, 0.32097283],\n",
       "        [0.05121979, 0.5590654 , 0.78117268, 0.64897132, 0.98347347,\n",
       "         0.27841603, 0.56809717, 0.07723502]]),\n",
       " array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_linears = [\n",
    "    torch.nn.Linear(data.shape[1], n_classes)\n",
    "    for _ in range(samples)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.01981 += 0.00830'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_torch_grad(torch_linear):\n",
    "    logits = torch_linear(torch.tensor(data, dtype=torch.float32))\n",
    "    dist = RelaxedOneHotCategorical(temp, logits=logits)\n",
    "    output = dist.rsample()\n",
    "    loss = torch.mean((output - torch.tensor(y)) ** 2)\n",
    "    loss.backward()\n",
    "    return torch_linear.weight.grad.numpy().max()\n",
    "\n",
    "grad_samples = [get_torch_grad(torch_linear) for torch_linear in torch_linears]\n",
    "f'{np.mean(grad_samples):.5f} += {np.std(grad_samples):.5f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.25640118, 0.7435988 ], dtype=float32)>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    tf.random.set_seed(42)\n",
    "    z = tfd.RelaxedOneHotCategorical(\n",
    "        temp, logits=tf.convert_to_tensor([0., 1.])).sample()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1661, 0.8339])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = RelaxedOneHotCategorical(temp, logits=torch.tensor([0., 1.])).rsample()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00683 += 0.00212'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tf_grad(torch_linear):\n",
    "    with tf.device('/CPU:0'):\n",
    "        tf_linear = tf.keras.layers.Dense(n_classes)\n",
    "\n",
    "        tf_linear.build((None, data.shape[1]))\n",
    "        tf_linear.set_weights([\n",
    "            torch_linear.weight.detach().numpy().T,\n",
    "            torch_linear.bias.detach().numpy()\n",
    "        ])\n",
    "\n",
    "        tf_data = tf.convert_to_tensor(data)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tf_logits = tf_linear(tf_data)\n",
    "            dist = tfd.RelaxedOneHotCategorical(temp, logits=tf_logits)\n",
    "            output = dist.sample()\n",
    "            loss = tf.reduce_mean(tf.pow((y - output), 2))\n",
    "\n",
    "        weight_gradient, _ = tape.gradient(loss, tf_linear.weights)\n",
    "        return weight_gradient.numpy().max()\n",
    "\n",
    "\n",
    "grad_samples = [get_tf_grad(torch_linear) for torch_linear in torch_linears]\n",
    "f'{np.mean(grad_samples):.5f} += {np.std(grad_samples):.5f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 10), dtype=float32, numpy=\n",
       " array([[-0.6142186 ,  0.6297692 ,  0.49465802, -0.40699455,  0.3843618 ,\n",
       "         -0.01220748, -0.06731844,  0.16490209,  0.7125151 , -0.35208547],\n",
       "        [-0.41142106,  0.5612997 ,  0.12182345, -0.28776225,  0.64473546,\n",
       "         -0.56076264,  0.39762443, -0.20173165,  0.27442896, -0.5084722 ],\n",
       "        [-0.56447244,  0.46228093,  0.44391337, -0.18342935,  0.14141229,\n",
       "          0.12823752, -0.17296311,  0.02412914,  0.56035423, -0.45750996],\n",
       "        [-0.6395828 ,  0.34457055,  0.45277998, -0.28159922, -0.03297274,\n",
       "          0.28652987, -0.20939389,  0.3001512 ,  0.6854204 , -0.19646247]],\n",
       "       dtype=float32)>,\n",
       " tensor([[-0.6142,  0.6298,  0.4947, -0.4070,  0.3844, -0.0122, -0.0673,  0.1649,\n",
       "           0.7125, -0.3521],\n",
       "         [-0.4114,  0.5613,  0.1218, -0.2878,  0.6447, -0.5608,  0.3976, -0.2017,\n",
       "           0.2744, -0.5085],\n",
       "         [-0.5645,  0.4623,  0.4439, -0.1834,  0.1414,  0.1282, -0.1730,  0.0241,\n",
       "           0.5604, -0.4575],\n",
       "         [-0.6396,  0.3446,  0.4528, -0.2816, -0.0330,  0.2865, -0.2094,  0.3002,\n",
       "           0.6854, -0.1965]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_linear = torch_linears[0]\n",
    "tf_linear = tf.keras.layers.Dense(n_classes)\n",
    "\n",
    "tf_linear.build((None, data.shape[1]))\n",
    "tf_linear.set_weights([\n",
    "    torch_linear.weight.detach().numpy().T,\n",
    "    torch_linear.bias.detach().numpy()\n",
    "])\n",
    "\n",
    "tf_linear(tf.convert_to_tensor(data)), torch_linear(torch.tensor(data, dtype=torch.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
